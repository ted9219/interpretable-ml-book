## Detecting Concepts 

`r if(is.html){only.in.html}`

<!--intro -->
This chapter presents techniques for analyzing which concepts a neural network learned.
Concept here means an abstract idea, which is pre-defined by a human.
While  [feature visualization](#feauture-visualization) tries to detect features from neural network units, which might match a concept (e.g. dog snouts) but doesn't have to, concept detection starts with a concept and analyzes how the neural network handles this concept.


<!-- why are concepts interesting? -->
Feature visualization is more explorational:
What does the neural network detect?
But it does not help when we have more concret questions, like how important was the concept of dog snouts for the classification?


<!-- Approaches that we will look at -->
We will look at two approaches: Network Dissection and Concept Activation Vectors.
Both approaches require additional labeling of data, but in different ways.

### TCAV: Testing with concept activation vectors.

But what about more implicit concepts?
Concepts for which we have no prior labeled data?

TCAV by Kim et al. (2019)[^tcav] explain a prediction by showing the importance of more high level concepts (e.g. texture, gender, color) for the prediction or classification.

You have to learn the concepts from data.
That means if you want to understand whether the network uses the concept of "female" for the classification of e.g. images, you have to provide some examples of "female" (could be images with women in it), and non-female (images without women in it).

You send all those images through the network 

Good thing is that TCAV does not require to change the network you are using, but you can use the network that you already have.




TCAV uses directional derivatives to quantify the importance of a concept for the classification or prediction.
The concept is defined by the user and must be defined via some positive and negative data examples.
For example for the image classification of a zebra, the concept might be stripes.
The concept is defined byselecting images of stripes and some randomly sampled images without stripes.

```{r tcav, fig.cap="Figure from TCAV paper, Been Kim et. al (2018) ", out.width=800}
knitr::include_graphics("images/tcav.png")
```


Code for TCAV: https://github.com/tensorflow/tcav

TODO: CONTINUE DESCRIBING TCAV

Good things about TCAV:
The concepts are not required to be known at training time.
Really any concept can be analyzed, as long as you find some positive and negative examples.

<!-- Feature Visualization for RNNs -->
For RNNs: https://medium.com/@plusepsilon/visualizations-of-recurrent-neural-networks-c18f07779d56
https://distill.pub/2019/memorization-in-rnns/
http://lstm.seas.harvard.edu/

TODO: Checkout RNNVis and LSTMVis

List of notebooks:
https://github.com/tensorflow/lucid
More a tool for getting a general, better understanding of cnns, but not for daily job.

### Word Embeddings

**Word Embeddings**
Word embeddings represent words as vectors which can be used to compute the similarity between words.
As another way to visualize concepts that were learned are word embeddings.
An embedding maps a discrete feature (e.g. a word) to a m-dimensional vector.
A word embedding is the vector in some embedding space a word is mapped onto.
The embedding space is learned by the neural network.
The directions in that space often correlate to concepts.
This means that words with similar vectors have some similarity, e.g. cat and dog.
This also has the nice effect that we can do arithmetics in that space.
e.g.

$$embedding(king)-embedding(queen)=embedding(man)-embedding(woman)$$

The embeddings are high-dimensional vectors.
For visualization, they are often mapped to 2 Dimensions (e.g. with tSNE) TODO: CITE

What can you do with embeddings?
You can visualize the concepts that were learned.
Embedding let us analyze what the neural network learned.
For example, did it learn some kind of bias?
How do we get word embeddings?
Other use cases include to use these embeddings as feature transformations before the e.g. text is used in a machine learning model.

How are they created?
It's a mapping from categorical features (e.g. words) to some vectors.
They can be initialized with random weights and the embeddings are learned along with the thing you are trying to predict, e.g. with a recurrent neural network.
An alternativ is to use a pre-trained embedding like word2vec, GloVe or fasttext.
Those are trained over huge corpuses of text to predict words from their neighboring words.


 - concepts can transform when learning, e.g. dog into waterfall

**Detecting Concepts During Training Time**

Towards Robust Interpretability with Self-Explaining Neural Networks


**Software**

- CAffee and with GANS https://github.com/Evolving-AI-Lab/synthesizing


### Other approaches for concepts

- Word embeddings https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
- 

[^TCAV]: Kim, Been, et al. "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)." arXiv preprint arXiv:1711.11279 (2017).

[^dissect]: Bau, David, et al. "Network dissection: Quantifying interpretability of deep visual representations." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.
